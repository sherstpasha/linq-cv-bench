

===== PAGE 1 =====
2025 
УТВЕРЖДЕН 
RU.РМДВ.04.06.014-03 34 01-ЛУ 
ФРЕЙМВОРК IV A H1 
Руководство оператора 
RU.РМДВ.04.06.014-03 34 01 
Листов 28

===== PAGE 2 =====
2 
RU.РМДВ.04.06.014-03 34 01 
АННОТАЦИЯ 
В настоящем программном документе приведено руководство оператора по 
работе с фреймворком IV A H1.  
В разделе 1 «Назначение программы» указаны сведения о назначении 
программы и ее функциях. 
В разделе 2 «Условия выполнения программы» указаны необходимые 
минимальные характеристики оборудования и программных средств для 
функционирования программы. 
В разделе 3 «Выполнение программы» указана информация о начальных 
настройках и библиотеках необходимых для работы программы, а также 
инструкция по с фреймворком IV A H1.  
В разделе 4 «Сообщения оператору» приведены тексты сообщений, 
выдаваемых в ходе выполнения программы и описание их содержания. 

===== PAGE 3 =====
3 
RU.РМДВ.04.06.014-03 34 01 
 
 
СОДЕРЖАНИЕ 
 
1 Назначение програмы…………………………………………………………….4 
1.1 Наименование и обозначение программы……………………………...4 
1.2. Назначение и области применения программы……………………….4 
2 Условия выполнения программы………………………………………………..5 
2.1 Требования к аппаратному и программному обеспечению, 
необходимым для функционирования программы……………………………….5 
2.2 Инструкция по установке ПО фреймворка IVA H1……………………5 
3 Выполнение программы………………………………………………………….7 
3.1 Python API: Quantization…………………………………………………7 
3.2 Python API: Compilation………………………………………………….9 
3.3 Инструкция по работе фреймворка……………………………………11 
4 Сообщения оператору…………………………………………………………...23 
Перечень терминов………………………………………………………………...25 
Перечень сокращений……………………………………………………………...26 
 
  

===== PAGE 4 =====
4 
RU.РМДВ.04.06.014-03 34 01 
 
 
1 НАЗНА ЧЕНИЕ ПРОГР АММЫ 
1.1 Наименование и обозначение программы 
1.1.1 Наименование — «Фреймворк IV A H1» ( далее –  
фреймворк IV A H1). 
1.1.2 Обозначение программы – RU.РМДВ.04.06.014-03. 
 
 
1.2 Назначение и области применения программы 
1.2.1 Функциональное назначение программного изделия: фреймворк IV A 
H1 RU.РМДВ.04.06.014-03 представляет собой доработанную и расширенную, 
по сравнению с предыдущей версией Фреймворка IV A H1   RU.РМДВ.04.06.014-
02, высокоуровневую библиотеку, предназначенную для работы  с тензорным 
процессором с архитектурой IV A H1. Основным назначением фреймворка IV A 
H1 является подготовка оригинальных графов нейронных сетей к запуску.  
1.2.2 Эксплуатационное назначение программного изделия: 
Разрабатываемый фреймворк IV A H1 может эксплуатироваться на любой ЭВМ, 
которая отвечает программным и аппаратным ограничениям проектируемого 
программного обеспечения. 
1.2.3 Фреймворк IV A H1 разработан для использования в отраслях, 
применяющих алгоритмы искусственных нейронных сетей, компьютерного 
зрения, распознавания по голосу, машинного обучения и других методов 
искусственного интеллекта.   

===== PAGE 5 =====
5 
RU.РМДВ.04.06.014-03 34 01 
 
 
2 УСЛОВИЯ ВЫПОЛНЕНИЯ ПРОГР АММЫ 
2.1 Требования к аппаратному и программному обеспечению, 
необходимым для функционирования программы 
2.1.1 Для корректного функционирования фреймворка IV A H1  к 
аппаратному обеспечению и системным программным средствам, используемым 
программой, предъявляются следующие характеристики: 
– Операционная система: Linux Ubuntu версии 20.04;  
– Процессор Intel(R) Core (TM) i7-7700 CPU @ 3.60GHz;  
– Оперативная память: не менее 32 ГБ; 
– Python 3.11. 
 
2.1.2 Краткое описание пакета поставляемых файлов 
В комплект программного обеспечения фреймворк IV A H1 входит архив, 
содержащий основные исполнительные файлы фреймворка IV A H1 (название 
архива может включать цифровой спецификатор версии) – «tpu_framework-py3-
none-any.whl». 
 
2.2 Инструкция по установке ПО фреймворка IV A H1 
2.2.1 Для выполнения процедуры установки ПО фреймворка IV A H1 
необходимо выполнить следующие действия: 
− открыть консоль/терминал операционной системы Linux Ubuntu; 
− перейти в директорию с архивом программы;  
− выполнить команды: 
1) python3 -m venv venv 
2) source venv/bin/activate 
3) pip install -U pip 
4) pip install tpu_framework-py3-none-any.whl 
Данная последовательность команд выполняет процедуру последующей 
установки с помощью пакетного менеджера Python.  

===== PAGE 6 =====
6 
RU.РМДВ.04.06.014-03 34 01 
 
 
2.2.2 О корректности выполнения процедуры установки и настройки ПО 
фреймворка IV A H1 информируют соответствующие сообщения установщика и 
отсутствие сообщений об ошибках. По итогам выполнения указанных шагов ПО 
фреймворка IV A H1 установлено и настроено для последующего использования. 
Для проверки необходимо выполнить следующую команду: 
python3 -c "import tpu_framework" 
Затем убедиться, что выходной код предыдущей команды равен нулю с 
помощью команды: 
echo $? 
В результате последняя строка вывода должна состоять из единственного 
символа ноля. 
 

===== PAGE 7 =====
7 
RU.РМДВ.04.06.014-03 34 01 
3 ВЫПОЛНЕНИЕ ПРОГР АММЫ 
3.1 Python API: Quantization 
QuantizedModel преобразует исходную модель пользователя в формате 
TensorFlow в квантованную модель во внутреннем формате . Если 
пользовательская модель хранится в формате ONNX, она предварительно 
конвертируется в пользовательскую модель в формате TensorFlow с помощью 
конвертера, а затем в квантованный граф в формате TensorFlow и квантованную 
модель в формате *. qm файла. Последняя содержит необходимые входные 
данные для последующей компиляции модели, а именно: топологию нейронной 
сети и соответствующие числовые данные. 
сlass QuantizedModel 
Родительский класс: ABC 
as_graph(batch_size=1, batch_axis=0, platform='64x64_fpga') 
Преобразует квантованную модель в GraphDef 
Параметры: 
batch_size – размер пакета  сэмплов для каждой операции инференса 
нейронной сети. 1 по умолчанию. 
batch_axis – номер оси пакетной обработки. 0 по умолчанию; 
platform (str) – параметр, указывающий, для какой платформы  
выполняется квантование. 128x128_asic по умолчанию 
Возвращаемый тип: GraphDef 
quantize (original_graph_def, input_shapes, output_nodes, calibration_dict, 
percentile=100.0, batch_axis=0, explicit_batch_value=None) 
Квантование указанной модели нейронной сети происходит следующим 
образом:   
1) Читается входной граф TF (конфигурация сети, веса);
2) Рассчитывается порог квантования на основе калибровочного тензора;

===== PAGE 8 =====
8 
RU.РМДВ.04.06.014-03 34 01 
 
 
3) Создается SoftwareNetwork ( содержит дополнительные операции 
квантования, дополнительное перемасштабирование, квантованные веса, 
операции с графом); 
4) Создается HardwareNetwork (модель TF, которая повторяет все операции 
на TPU); 
Параметры: 
original_graph_def (GraphDef) — Исходный граф в формате 
tf.compat.v1.GraphDef ; 
input_shapes (Union[str, Mapping[str, Tuple[Optional[int], ...]]]) – 
описание входных узлов; 
output_nodes (Union[List[str], List[Dict[str, Any]], None]) – описание 
выходных узлов; 
calibration_dict (Dict[str, ndarray]) – предварительно подготовленный 
словарь для определения порогов квантования 
percentile (float)  – значение, которое определяет процент весовых 
коэффициентов, ограничиваемых по величине 
batch_axis (int) – номер оси пакетной обработки 
explicit_batch_value ( Optional[ int]) – конкретное значение размера 
пакета, необходимое для сетей, которые не поддерживают неявный размер 
пакета. 
Тип возврата: _InMemoryQuantizedModel 
rescale(data) 
Подготовка входных данных для квантованного графа или преобразование 
выходных данных в исходный формат 
Параметры: 
data (Dict[str, ndarray]) – словарь, содержащий имя тензоров и данные для 
преобразования. 
save(file_path) 
Сохранение квантованной модели по пути к файлу. 

===== PAGE 9 =====
9 
RU.РМДВ.04.06.014-03 34 01 
 
 
Параметры: 
file_path ( str) – путь к файлу для дампа. 
 
3.2 Python API: Compilation 
Компилятор выполняет компиляцию квантованной модели в программу 
для TPU2. 
compile_(hardware_parameters,network,parameters,slicing_scheme=None) 
Родительский класс: 
Параметры: 
hardware_parameters (ModelParameters) – параметры модели 
network (Network) – внутреннее представление квантованной модели 
parameters (CompileParameters) – параметры компиляции 
slicing_scheme (Optional[Dict[str, Tuple[Dict[TensorAxis, slice], ...]]]) – 
схема распределения тензоров в памяти  микропроцессора IV A H1. 
Тип возврата: Tuple[ Executable, Dict[ str, TensorDescription]] 
Компилятор поддерживает различные типы платформ. Функция 
.get_hw_params используется для получения hardware_parameters платформы  
(например, 128x128_asic). 
get_hw_params(tpu_type) 
Родительский класс: 
Параметры: 
tpu_type (str) – строковый идентификатор платформы 
Тип возврата: ModelParameters 
parameters(CompileParameters) 
Компилятор поддерживает два предустановленных набора параметров 
компиляции: 
1) DEFAULT; 
2) STAGING. 

===== PAGE 10 =====
10 
RU.РМДВ.04.06.014-03 34 01 
Пресет STAGING позволяют выполнять следующие оптимизации 
компиляции: 
1) Разрешить параллельное выполнение конвейеров TPU.
2) Использовать второй DDR для загрузки констант параллельно как с
DTL0, так и с DTL1. 
3) Более эффективно размещать тензоры в кеше: разрешить размещать
одни тензоры в тех же банках памяти, что и другие, если это не мешает 
выполнению; заранее выделить место, чтобы обеспечить предварительную 
загрузку; продлите срок пребывания тензора в памяти , чтобы отсрочить его 
выход за пределы области действия, что позволит некоторым конвейерам 
работать дольше, не требуя перехода кеша в следующее фиксированное 
состояние. 
4) Включить слияние слоев: если какой -то подграф A→B вычисляется в
частичной форме как A1 и A2 и B1 и B2, может быть возможно вычислить B1 
непосредственно из A1 без необходимости вычисления A2. 
5) Слайсер слоев делает схемы размещения тензоров в памяти более
гибкими, чтобы обеспечить больше возможных слияний слоев. 
Примечание - Компиляция в режиме STAGING может иногда давать 
недопустимые программы или компилировать слишком долго. Если это так, 
необходимо вернуться к предыдущей версии предустановок DEFAULT и 
отправить отчет об ошибке. 
 DEFAULT 
Достаточно безопасная оптимизация, которая считается стандартной для 
новой сети. 
STAGING 
Получить промежуточные (наиболее оптимизированные) параметры 
компилятора. Промежуточные параметры могут иногда давать неверные 
программы или компилировать их слишком долго. Если это так, вернитесь к 
предустановкам ПО УМОЛЧАНИЮ или ОТКЛЮЧЕННЫМ предустановкам. 

===== PAGE 11 =====
11 
RU.РМДВ.04.06.014-03 34 01 
 
 
3.3 Python API: Converter 
Выполняет конвертацию модели нейронной сети в формате onnx в формат 
Tensorflow, возвращая сконвертированный граф и служебный словарь 
переименований слоев нейронной сети: 
onnx_to_tf(onnx_model: onnx.ModelProto) 
Параметры: 
onnx_model:  onnx.ModelProto – объект графа нейронной сети 
 Тип возврата: Tuple[tensorflow.Graph, Dict[str, str]] 
 
3.4 Инструкция по работе фреймворка 
В настоящем разделе работа фреймворка IV A H1 показана на примере 
решения задачи распознавания объекта на изображении (задача классификации). 
Сэмплом является изображение в формате JPEG. В качестве модели нейронной 
сети используется resnet50 в формате ONNX. Для работы с изображениями 
используется би блиотека Pillow (PIL), которая устанавливается командой в 
терминале: 
pip install Pillow 
Для работы теста в программу импортируются ранее установленные 
библиотеки предварительной и последующей обработки glob, os, tempfile, 
numpy, numpy.typing, pytpu, tensorflow и др. 
Из библиотеки typing импортируются сущности Any и Dict. 
Из библиотеки tpu_framework импортируются сущности DEFAULT, 
Network, TPU_128x128_PARAMS, TpuProgram, compiler, dnn_quant. 
Из библиотеки onnx_direct импортируется функция onnx_to_tf. 
Перед началом работы с фреймворком пользователю необходимо ввести в 
программу данные, которые он в последствии будет использовать для 
выполнения своей конкретной задачи и результата, который он хочет получить в 
процессе эксплуатации фреймворка IV A H1. Для этого ему необходимо:  

===== PAGE 12 =====
12 
RU.РМДВ.04.06.014-03 34 01 
1) В переменную GRAPH_PATH записать путь к директории с исходной
моделью. Например, входным файлом ONNX torch_resnet50.onnx: 
GRAPH_PATH = '/auto/tests/frozen_graphs/resnet_50.onnx' 
2) В переменную IMAGENET_IMAGES_BASE_PATH записать путь к
папке с набором данных, например, изображений: 
IMAGENET_IMAGES_BASE_PATH='/auto/datasets/imagenet/ILSVRC2012
_img_val' 
3) В переменную DEMO_IMAGE_FILE_NAME записывается путь к
данным с известным (ожидаемым) классом. В примере используется демо 
входное изображение: 
DEMO_IMAGE_FILE_NAME=os.path.join(IMAGENET_IMAGES_BASE_
PATH, 'ILSVRC2012_val_00000045.JPEG') 
4) Переменной DEMO_IMAGE_EXPECTED_CLASS присваивается
значение, присвоенное ожидаемым данным на постпроцессинге. В примере 
таким значением является номер присвоенный ожидаемому изображению 
равный 257:  
DEMO_IMAGE_EXPECTED_CLASS = 257 
5) Переменной CALIBRATION_IMAGES присваивается путь для чтения
данных калибровки. В примере это изображение, используемое для калибровки: 
CALIBRATION_IMAGES=os.path.join(IMAGENET_IMAGES_BASE_PAT
H, "*.JPEG") 
6) Переменной INPUT_TENSOR_NAME присваивается имя входного
тензора для исходной модели input 1: 
INPUT_TENSOR_NAME = ' input:1' 
7) Переменной OUTPUT_TENSOR_NAME присваивается имя выходного
тензора исходной модели: 
OUTPUT_TENSOR_NAME = 'logits:0' 
Затем объявляется функция предварительной обработки, параметры 
которой предоставляется пользователем вместе с оригинальной моделью. В 

===== PAGE 13 =====
13 
RU.РМДВ.04.06.014-03 34 01 
примере представлены параметры для преобразования входного изображения во 
входной тензор и добавляется batch ось:  
def preprocess(image: Image) -> npt.NDArray[np.float32]: 
image = image.convert('RGB') 
    width, height = image.size 
    min_dim = int(min(height, width) * 0.85) 
    crop_top = (height - min_dim) // 2 
    crop_left = (width - min_dim) // 2 
    image = image.crop((crop_left, crop_top, crop_left + min_dim, crop_top + 
min_dim)) 
    image = image.resize(size=(224, 224)) 
    tensor: npt.NDArray[Any] = np.asarray(image).astype(np.float32) 
    assert tensor.ndim == 3 
    tensor = tensor - np.array([123.68, 116.779, 103.939], dtype=np.float32) 
    return np.expand_dims(tensor, axis=0)   
После объявляется функция постобработки для преобразования выходного 
тензора в интерпретируемый объект (в этом примере в индекс наиболее 
вероятный класс). Она так же предоставляется пользователем вместе с 
оригинальной моделью:  
def postprocess(tensor: np.ndarray) -> np.ndarray: 
return np.argmax(tensor) + 1   
Затем объявляется вспомогательная функция для одного запуска сеанса 
TensorFlow с заданным graph_def, другие позиционные и именованные 
аргументы передаются дальше. Эта функция нужна для генерации эталонных 
данных для проверки результатов запуска нейронной сети на ТПУ: 
def _tensorflow_inference(graph_def: tf.compat.v1.GraphDef, *args, **kwargs) 
-> np.ndarray: 
После происходит преобразование графа исходной модели из GraphDef в 
tf.Graph: 

===== PAGE 14 =====
14 
RU.РМДВ.04.06.014-03 34 01 
graph = tf.graph_util.import_graph_def(graph_def) 
with tf.compat.v1.Session(graph=graph) as sess: 
 return sess.run(*args, **kwargs) 
После объявляется функция загрузки исходной модели графа из места, 
определенного ранее. Если модель пользователя представлена в формате 
tensorflow, то функция _read_original_model() выглядит следующим образом:  
def _read_original_model() -> tf.compat.v1.GraphDef: 
with tf.compat.v1.io.gfile.GFile(GRAPH_PATH, 'rb') as file: 
 file_data = file.read() 
original_graph_def = tf.compat.v1.GraphDef() 
 original_graph_def.ParseFromString(file_data) 
 return original_graph_def 
Если модель пользователя представлена в формате onnx, то функция 
_read_original_model() выглядит следующим образом: 
def _read_original_model() -> onnx.ModeProto: 
return onnx.load(GRAPH_PATH) 
Загружается словарь входных данных (ключ — это имя входа модели, а 
значение — входной тензор). Загружается тестовое изображение:  
  with Image.open(DEMO_IMAGE_FILE_NAME) as demo_image: 
Примечание - Входное изображение представляет собой 
PIL-изображение « demo_image», но входное «тензорное изображение» 
представляет собой предварительно обработанное входное изображение: 
return {INPUT_TENSOR_NAME: preprocess(demo_image)} 
Выполняется функция загрузки нагрузочных тензоров, используемых для 
калибровки сети во время процедуры квантования: 
def _load_calibration_tensor() -> Dict[str, np.ndarray]: 
Подготовка калибровочных тензоров: 
number_of_calibration_tensors = 2 
calibration_tensor_list = [] 

===== PAGE 15 =====
15 
RU.РМДВ.04.06.014-03 34 01 
В этом тесте для простоты формирования калибровочного тензора 
использовались 2 изображения: 
calibration_images=glob.glob(CALIBRATION_IMAGES)[:number_of_calibra
tion_tensors] 
Преобразование калибровочного изображения в калибровочный тензор: 
for image_path in calibration_images: 
with Image.open(image_path) as image: 
tensor = preprocess(image) 
calibration_tensor_list.append(tensor) 
Каждый элемент calibration_tensor_list имеет формат NHWC, 
контактирующий с пакетами axis = 0: 
calib_tensor = np.concatenate(calibration_tensor_list, axis=0, dtype=np.float32) 
return {INPUT_TENSOR_NAME: calib_tensor} 
Выполнение функции проверки исходного поведения модели с помощью 
тестового прогона с использованием tensorflow: 
def _test_original_model(original_model_graph_def: tf.compat.v1.GraphDef, 
test_input_dict: Dict[str, np.ndarray]) -> None: 
Проверка инференса с оригинальной моделью:  
original_graph_output_tensors = _tensorflow_inference( 
        graph_def=original_model_graph_def, 
        feed_dict=test_input_dict, 
        fetches=OUTPUT_TENSOR_NAME 
    ) 
Для удобства все входы и выходы хранятся в виде словарей (ключ — имя 
тензора): 
original_graph_output_dict={OUTPUT_TENSOR_NAME:original_graph_out
put_tensors} 
Убедиться, что исходная модель ведет себя так, как ожидалось: 

===== PAGE 16 =====
16 
RU.РМДВ.04.06.014-03 34 01 
 
 
_assert_output_dictionary(original_graph_output_dict, 'Original model') 
Объявление функции проверки поведения квантованной модели с 
помощью тестового прогона используя tensorflow: 
def _test_quantized_model(quantized_model: dnn_quant.QuantizedModel, 
test_input_dict: Dict[str, np.ndarray]) -> None: 
Квантованная модель хранит информацию внутри пользовательского 
формата и содержит ссылку на реализацию TensorFlow.  
Примечание - Квантованная модель пакетно -чувствительная (на вход 
модели можно подать определенное кол -во изображений, которое явно 
определенно в графе): 
quantized_model_graph_def = quantized_model.as_graph( 
batch_size=1 
batch_axis=0, 
    ) 
Входное квантование — это умножение входных данных на коэффициент 
масштабирования квантования, и затем приведение к нужному типу данных, 
который определяется набором операций в квантованном графе: 
input_dict = quantized_model.rescale(test_input_dict) 
Выполните запуска теста с использованием библиотеки tensorflow: 
output_tensor = _tensorflow_inference( 
        graph_def=quantized_model_graph_def, 
        feed_dict=input_dict, 
        fetches=OUTPUT_TENSOR_NAME 
    ) 
Выходное деквантование — это умножение входных данных на 
квантование, коэффициент масштабирования, а затем приведение к 
пользовательскому формату (например float32): 
output_tensor = quantized_model.rescale ({OUTPUT_TENSOR_NAME: 
output_tensor}) 

===== PAGE 17 =====
17 
RU.РМДВ.04.06.014-03 34 01 
Для того что бы убедиться, что квантованная модель ведет себя так, как 
ожидалось выполняется следующая команда: 
assert_output_dictionary(output_tensor, 'Quantized model') 
SUFFIX = ':0' – Имя тензора суффикс, который можно добавить/удалить 
Объявление вспомогательной функции для удаления нулевого суффикса из 
имени тензора и получения OP имени: 
def _remove_zero_suffix(tensor_name: str) -> str: 
if tensor_name.endswith(_SUFFIX): 
        return tensor_name[:-len(_SUFFIX)] 
    else: 
        return tensor_name 
Объявление функции добавления нулевого суффикса к имени тензора, 
таким образом получив имя тензора из OP имени:  
def _add_zero_suffix(op_name: str) -> str: 
if op_name.endswith(_SUFFIX): 
        return op_name 
    else: 
        return op_name + _SUFFIX 
Заменить имена тензоров на ОР имена: 
test_input_dict_with_op_names = {_remove_zero_suffix(tensor_name): 
tensor_value 
 for tensor_name, tensor_value in test_input_dict.item 
Выполнить на ТLM: 
tlm_result = model_execute( 
        executable=executable, 
        tensor_descriptions=tensor_descriptions, 
        input_data=test_input_dict_with_op_names, 
    ) 
print('TLM estimates execution time:', tlm_result.execution_time_us, 'us') 

===== PAGE 18 =====
18 
RU.РМДВ.04.06.014-03 34 01 
Замена выходных ОР имен на оригинальные тензорные имена с 
добавлением нулевого-суффикса:  
output_dict_with_tensor_names = {_add_zero_suffix(op_name): tensor_value 
for op_name, tensor_value in tlm_result.output_data.items()} 
_assert_output_dictionary(output_dict_with_tensor_names, 'TLM') 
Выполняется функция, которая изучает ранее заявленный словарь, чтобы 
убедиться, что это действительный результат модели (предсказан правильный 
класс): 
def _assert_output_dictionary(output_dict: Dict[str, np.ndarray], 
executor_name: str) -> None: 
Получить наиболее вероятный класс: 
  most_probable_class=postprocess(output_dict[OUTPUT_TENSOR_NAME]) 
Вывод результатов: 
print(f'{executor_name} reports most probable class estimate: 
{most_probable_class}') 
Выполняется проверка корректности индекса класса изображения, 
вычисленного моделью: 
assert most_probable_class == DEMO_IMAGE_EXPECTED_CLASS, \ 
f'Executor {executor_name} predicted wrong class {most_probable_class}'\ 
f' (expected class is {DEMO_IMAGE_EXPECTED_CLASS})' 
Выводится дополнительная отладочная информация о полученных данных 
исполнителя, чтобы подчеркнуть возможные различия между выведенной 
информацией различных исполнителей: 
  print(f'{executor_name} TOP -16 argsort output is: 
{np.flip(np.argsort(output_dict[OUTPUT_TENSOR_NAME][0]))[:16]}') 
Выполняется функция сквозного тестирования, демонстрирующая полный 
маршрут от исходной модели до квантования, компиляции и исполнения на ТПУ: 
def test_end_to_end(): 
Загрузка в тест входных данных: 

===== PAGE 19 =====
19 
RU.РМДВ.04.06.014-03 34 01 
 
 
test_input_dict = _load_test_input_tensor_dict() 
Пользователь предоставляет исходную модель в виде экземпляра класса 
tensorflow или onnx, в данном случае мы читаем модель из файла: 
original_graph_def = _read_original_model() 
Если модель пользователя представлена в формате onnx, то ее необходимо 
дополнительно сконвертировать в формат tensorflow с помощью конвертера: 
original_graph_def = onnx_to_tf(original_graph_def) 
Демонстрируется работа оригинальной модели: 
_test_original_model(original_graph_def, test_input_dict) 
Получение калибровочных тензоров: 
calibration_dict = _load_calibration_tensor()   
Квантование оригинальной модели: 
quantized_model = dnn_quant.QuantizedModel.quantize( 
        original_graph_def=original_graph_def, 
        calibration_dict=calibration_dict, 
        input_shapes={INPUT_TENSOR_NAME: (1, 224, 224, 3)}, 
        output_nodes=[OUTPUT_TENSOR_NAME], 
) 
У даление калибровочных данных: 
del calibration_dict 
Демонстрация рабочей квантованной модели: 
_test_quantized_model(quantized_model, test_input_dict) 
Определяются параметры целевого оборудования (экземпляр класса 
ModelParameters), параметры могут быть созданы вручную или использоваться 
предопределенный набор: 
hardware_parameters = TPU_128x128_PARAMS 
Компилятору требуется модель в другом формате (эта модель не 
обязательно приходит от утилиты квантования) поэтому требуется ручное 
преобразование: 

===== PAGE 20 =====
20 
RU.РМДВ.04.06.014-03 34 01 
network = Network.from_quantized_model(quantized_model) 
Компиляция квантованной модели. Компиляция выдает так называемую 
программу TLM которая содержит все TPU инструкции и константы (веса).  
Скомпилированная программа должна быть сохранена в отдельный файл 
.tpu (внутренний формат). 
Скомпилированная TPU программа является разделительной линией 
между TPU Framework ( которая имеет исходную модель на входе и программу 
TPU на выходе) и TPU SDK (который имеет программу TPU в качестве входных 
данных и позволяет пользователю делать инференс с заданными данными).  
with tempfile.TemporaryDirectory() as temporary_directory: 
Полный путь к  скомпилированной программе (файлу .tpu):  
tpu_program_file_name = os.path.join(temporary_directory, 'program.tpu') 
Сохранение TLM программ в TPU формате: 
tpu_program = TpuProgram.from_executable(*tlm_program) 
tpu_program.to_file(tpu_program_file_name) 
Получение списка всех доступных TPU устройств в локальном хосте: 
  available_tpu_devices = tpu.Device.list_devices() 
        assert available_tpu_devices, 
Открываем первое доступное TPU устройство: 
with tpu.Device.open(available_tpu_devices[0]) as tpu_device: 
Загружаем сохраненную программу на TPU: 
with tpu_device.load(tpu_program_file_name) as tpu_program: 
Создаем сеанс инференса: 
with tpu_program.inference() as inference: 
Заменяем добавленное имя тензора на добавленное OP имя: 
input_dict_with_op_names = { 
    _remove_zero_suffix(INPUT_TENSOR_NAME): 
test_input_dict[INPUT_TENSOR_NAME], 
      } 

===== PAGE 21 =====
21 
RU.РМДВ.04.06.014-03 34 01 
Выполнить один инференс (блокировку вызова): 
tpu_output = inference.sync(input_dict_with_op_names) 
Поменять на выходе ОР имя на имя тензора: 
tpu_output_with_tensor_names = { 
    OUTPUT_TENSOR_NAME: 
tpu_output[_remove_zero_suffix(OUTPUT_TENSOR_NAME)] 
      } 
Выполнить проверку того, что TPU (как альтернативная модель) работает. 
Обратить внимание, что выходной словарь TPU ( а также выходные данные 
исходной модели и выходные данные квантованной модели) подвергнуты той же 
процедуре испытаний: 
_assert_output_dictionary(tpu_output_with_tensor_names, 'TPU') 
3.4 Список поддерживаемых нейронных сетей 
Далее представлен список нейронных сетей,  протестированных с 
использованием фреймворка IV A H1: 
− FCN_8s (нейронная сеть класса U-Net); 
− lenet5; 
− vgg19; 
− inception_v1; 
− inception_v2; 
− resnet50; 
− mobilenet_v1; 
− mobilenet_v2; 
− tiny_yolo2; 
− yolo2; 
− tiny_yolo3; 
− yolo3; 
− resnet50_v2; 

===== PAGE 22 =====
22 
RU.РМДВ.04.06.014-03 34 01 
 
 
− seresnet50; 
− flownet;  
− REGNET_Y_800MF; 
− anti_spoof_mn;  
− yolo5s. 
  

===== PAGE 23 =====
23 
RU.РМДВ.04.06.014-03 34 01 
4 СООБЩЕНИЯ ОПЕР АТОРУ 
В процессе работы end-to-end теста в консоль может выдаваться 
отладочная информация, содержащая служебную информацию от компонентов 
фреймворка: конвертера, утилиты квантования и компилятора. Например: 
… 
23:34:28 INFO Start creating graph 
23:34:28 INFO <Logger CalibrationNetwork (DEBUG)>:Create "Placeholder" 
node with type "input_node" 
23:34:28 WARNING From /opt/CI/gitlab -runner/builds/xS4F6Gi-
/0/tpu_sw/tpu_framework/venv/lib/python3.8/site-
packages/dnn_quant/network/nodes/meta_nodes/calibr_metanodes/base_calibr_meta
node.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is 
deprecated and will be removed in a future version. 
Instructions for updating: 
Call initializer instance with the dtype argument instead of passing it to the 
constructor 
23:34:28 DEBUG Output_node shape: "[1, 224, 224, 3]" 
23:34:28 INFO <Logger CalibrationNetwork (DEBUG)>:Create 
"resnet_v1_50/Pad" node with type "pad_layer" 
… 
Уровень логгирования  может быть настроен пользователем 
самостоятельно с помощью стандартных компонентов языка Python (библиотека 
logging). При определенных настройках отладочный вывод может отсутствовать 
полностью. Таким образом, и наличие, и отсутствие данного вывода являетс я 
штатным поведением ПО и говорит о нормальной работе системы.  

===== PAGE 24 =====
24 
RU.РМДВ.04.06.014-03 34 01 
Независимо от наличия или отсутствия вывода отладочной информации в 
консоль, end-to-end тест будет завершен выводом в консоль следующей 
информации: 
Original model reports most probable class estimate: 257 
Original model TOP-16 argsort output is: [256 244 260 233 205 200 197 226 
252 255 247 220 185 199 206 224] 
Quantized model reports most probable class estimate: 257 
Quantized model TOP-16 argsort output is: [256 244 205 260 233 255 220 200 
197 247 226 206 257 209 207 185] 
TLM estimates execution time: 5627.364001024086 us 
TLM reports most probable class estimate: 257 
TLM TOP-16 argsort output is: [256 244 260 205 233 197 200 255 247 206 220 226 
252 209 199 219] 
TPU reports most probable class estimate: 257 
TPU TOP-16 argsort output is: [256 244 260 205 233 197 200 255 247 206 220 
226 252 209 199 219]с 
Во всех выводимых массивах значимым элементом является только 
нулевой элемент, по которому определяется класс изображения. Конкретные 
числовые значения остальных элементов могут отличаться.  

===== PAGE 25 =====
25 
RU.РМДВ.04.06.014-03 34 01 
ПЕРЕЧЕНЬ ТЕРМИНОВ 
Инференс — процесс выполнения обученной модели нейронной сети. 
Исходный граф нейронной сети – Нейронная сеть с обученными весами, 
которую планируется запускать на TPU, замороженная и сохраненная в форматах 
protobuf или onnx. 
Квантование – Последовательное масштабирование и округление данных, 
необходимое для представления данных в форматах пониженной разрядности, а 
также замена одних операций на другие, которые подходят ТПУ . 
Квантованная модель (*. qm) — архив, содержащий конфигурацию  
константы квантованной  сети. 
Квантованный граф нейронной сети – Исходный граф нейронной сети, 
прошедший этап квантования и сгенерированный с помощью dnn_quant. Обычно 
называется quant_network_graph.pb. 
Калибровочный тензор – Ndarray, содержащий N выборок из обучающего 
набора данных, предварительно обработанных так же, как и для обучения. 
Рекомендуемое количество образцов – 100. 
Компиляция – Преобразование файла конфигурации и констант 
квантованной нейронной сети в файл программы IV A TPU. 
Программный файл IV A TPU – Набор машинных инструкций для 
выполнения исходного графа нейронной сети на тензорном процессоре.  
Слой – Группа операций, которая представляет собой автономную 
структуру для вычисления высокоуровневых операций, таких как свертка, 
умножение матриц, объединение, транспонирование и т. д. 

===== PAGE 26 =====
26 
RU.РМДВ.04.06.014-03 34 01 
ПЕРЕЧЕНЬ СОКР АЩЕНИЙ 
IV A H1 – Device Framework
IV A TPU – Семейство тензорных микропроцессоров,
разрабатываемых ООО «ХайТэк», которые
предназначены для ускоренного расчета нейронных
сетей. Архитектура данного вида процессоров
оптимизирована для выполнения задач ускоренного
расчета нейронных сетей. Основой процесс оров
является блок матричного умножения, который
выполняет наиболее ресурсоемкие вычисления со
скоростью десятки тысяч операций за такт.
Специализированная архитектура процессора и
вычислительных элементов позволяет получить
более высокую производительност ь и
энергоэффективность по сравнению с другими
решениями, такими как многоядерные цифровые
сигнальные процессоры и графические карты.
Семейство процессоров включает в себя:
• IV A TPU – настраиваемый модуль (IP-блок)
тензорного ускорителя, пригодный для 
изготовления в ИС; 
• IV A TPU for FPGA – параметризированное
тензорное ядро для интеграции в 
специализированные системы на базе FPGA. 
TensorFlow – Открытая программная библиотека для машинного
обучения, разработанная компанией Google для
решения задач построения и тренировки нейронной
сети с целью автоматического нахождения и

===== PAGE 27 =====
27 
RU.РМДВ.04.06.014-03 34 01 
классификации образов, достигая качества 
человеческого восприятия. 
TPU – Tensor processor unit. Тензорный процессор,
относящийся к классу нейронных процессоров,
являющийся специализированной интегральной
схемой.